{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#elroy-an-ai-memory-and-reminder-assistant","title":"Elroy: An AI memory and reminder assistant","text":"<p>Elroy is an AI assistant that runs in your terminal, with memory and reminder tracking. It remembers everything you tell it, can learn from your documents, and helps you stay organized.</p> <p></p>"},{"location":"index.html#features","title":"Features","text":"<ul> <li>Reminders: Create, update, and track reminders, based on timing or context</li> <li>Memory: Elroy automatically recalls relevant information from past conversations</li> <li>Document Understanding: Ingest your docs to give Elroy context about your projects</li> <li>Simple Scripting: Automate tasks with minimal configuration overhead</li> <li>CLI Tool Interface: Quickly review memories or jot notes for Elroy to remember</li> <li>MCP Server: Surface conversation memories to other tools via Model Context Protocol</li> </ul> <p>The fastest way to get started is using the install script:</p> <pre><code>curl -LsSf https://raw.githubusercontent.com/elroy-bot/elroy/main/scripts/install.sh | sh\n</code></pre> <p>Or install manually with UV:</p> <pre><code># Install UV first\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Then install Elroy\nuv pip install elroy\n</code></pre> <p>For detailed installation instructions including Docker and source installation options, see the Installation Guide.</p>"},{"location":"index.html#elroy-from-your-terminal","title":"Elroy from your terminal","text":"<p>Elroy runs in your terminal helps you with reminders while maintaining memory of your interactions. As you chat with Elroy, it automatically:</p> <ol> <li>Creates memories of important information</li> <li>Recalls relevant context when needed</li> <li>Tracks reminders you set together</li> <li>Consolidates redundant information to keep context clean</li> </ol>"},{"location":"index.html#quickstart","title":"Quickstart","text":"<p>Run the install script: <pre><code>curl -LsSf https://raw.githubusercontent.com/elroy-bot/elroy/main/scripts/install.sh | sh\n</code></pre></p> <p>Ensure your env has <code>ANTHROPIC_API_KEY</code>, <code>OPENAI_API_KEY</code> or whatever model provider token you wish to use is set.</p> <pre><code># Start the chat interface\nelroy\n\n# Run with a specific model\nelroy --chat-model \"gemini/gemini-2.0-flash\"\n\n# To see more detailed CLI options\nelroy --help\n</code></pre>"},{"location":"index.html#slash-commands","title":"Slash Commands","text":"<p>Elroy's CLI supports slash commands for quick actions. Some examples (run <code>/help</code> to see the full list):</p> <pre><code># Create a memory\n/create_memory This is important information I want to save\n\n# Create a reminder\n/create_reminder Learn how to use Elroy effectively\n\n# Process a single message and exit\nelroy message \"Say hello world\"\n\n# Force use of a specific tool\nelroy message \"Create a reminder\" --tool create_reminder\n</code></pre>"},{"location":"index.html#scripting-with-elroy","title":"Scripting with Elroy","text":"<p>Elroy can be used in scripts and automated workflows:</p> <pre><code>from elroy import Elroy\n\nai = Elroy()\n\n# Create a memory\nai.remember(\"Important project context\")\n\n# Process a message with memory augmentation\nresponse = ai.message(\"What should I do next on the project?\")\nprint(response)\n</code></pre>"},{"location":"index.html#supported-models","title":"Supported Models","text":"<p>Elroy works with:</p> <ul> <li>OpenAI: GPT-4o, GPT-4o-mini, o1, o1-mini</li> <li>Anthropic: Claude 3.5 Sonnet, Claude 3 Opus</li> <li>Google: Gemini</li> <li>Any OpenAI-compatible API</li> </ul> <p>Under the hood, Elroy uses LiteLLM to interface with model providers.</p>"},{"location":"index.html#community","title":"Community","text":"<p>Come say hello!</p> <ul> <li>Discord</li> <li>GitHub</li> </ul>"},{"location":"index.html#license","title":"License","text":"<p>Distributed under the Apache 2.0 license. See LICENSE for more information.</p>"},{"location":"cli.html","title":"CLI","text":"<p>Elroy provides a powerful command-line interface that makes it easy to interact with the AI assistant directly from your terminal.</p>"},{"location":"cli.html#basic-usage","title":"Basic Usage","text":"<pre><code># Start the chat interface\nelroy chat\n\n# Or just 'elroy' which defaults to chat mode\nelroy\n\n# Process a single message and exit\nelroy message \"Say hello world\"\n\n# Create a memory\nelroy remember \"This is important information I want to save\"\n</code></pre>"},{"location":"cli.html#slash-commands","title":"Slash Commands","text":"<p>Elroy supports powerful slash commands for quick actions:</p> <pre><code># Create a memory\n/create_memory This is important information I want to save\n\n# Create a reminder\n/create_reminder Learn how to use Elroy effectively\n</code></pre> <p>For a full list of available tools and slash commands, see the Tools Guide.</p>"},{"location":"cli.html#command-reference","title":"Command Reference","text":"Command Description <code>elroy chat</code> Opens an interactive chat session (default command) <code>elroy message TEXT</code> Process a single message and exit. Use <code>--plain</code> for plaintext output instead of rich text <code>elroy remember [TEXT]</code> Create a new memory from text or interactively <code>elroy ingest PATH</code> Ingests document(s) at the given path into memory. Can process single files or directories <code>elroy list-models</code> Lists supported chat models and exits <code>elroy list-tools</code> Lists all available tools <code>elroy print-config</code> Shows current configuration and exits <code>elroy version</code> Show version and exit <code>elroy print-tool-schemas</code> Prints the schema for a tool and exits <code>elroy set-persona TEXT</code> Set a custom persona for the assistant <code>elroy reset-persona</code> Removes any custom persona, reverting to the default <code>elroy show-persona</code> Print the system persona and exit <code>elroy mcp</code> MCP server commands"},{"location":"cli.html#document-ingestion","title":"Document Ingestion","text":"<p>Elroy supports ingesting documents to make their content available for memory and retrieval:</p> <pre><code># Ingest a single file\nelroy ingest document.md\n\n# Ingest all files in a directory\nelroy ingest ./documents/\n\n# Ingest recursively with pattern matching\nelroy ingest ./documents/ --recursive --include \"*.md,*.txt\" --exclude \"*.log\"\n</code></pre>"},{"location":"cli.html#ingest-command-options","title":"Ingest Command Options","text":"Option Description <code>--force-refresh, -f</code> If true, any existing ingested documents will be discarded and re-ingested <code>--recursive, -r</code> If path is a directory, recursively ingest all documents within it <code>--include, -i</code> Glob pattern for files to include (e.g., '.txt,.md'). Multiple patterns can be comma-separated <code>--exclude, -e</code> Glob pattern for files to exclude (e.g., '*.log'). Can also be used to exclude directories"},{"location":"cli.html#shell-integration","title":"Shell Integration","text":"<p>Elroy can be used in scripts and automated workflows:</p> <pre><code># Process a single question\necho \"What is 2+2?\" | elroy chat\n\n# Create a memory from file content\ncat meeting_notes.txt | elroy remember\n\n# Use a specific tool with piped input\necho \"Buy groceries\" | elroy message --tool create_reminder\n</code></pre>"},{"location":"how_it_works.html","title":"How It Works","text":"<p>Elroy is an AI assistant that remembers conversations and helps you achieve your goals. This page explains the core concepts and architecture behind Elroy.</p>"},{"location":"how_it_works.html#core-concepts","title":"Core Concepts","text":""},{"location":"how_it_works.html#creating-memories","title":"Creating memories","text":"<p>As you chat with Elroy, it creates and stores memories. Later, when the text of these memories are semantically similar to your conversation, the assistant is reminded of the memory and prompted to reflect on how it pertains to your conversation.</p>"},{"location":"how_it_works.html#consolidating-and-updating-memories","title":"Consolidating and updating memories","text":"<p>Over time, memories can become outdated or redundant with other memories.</p> <p>If Elroy finds that the text of a memory is no longer accurate, it appends an update.</p> <p>In the background, Elroy's memory consolidation system combines and reorganizes memories, to make sure they are concise and unique as possible.</p>"},{"location":"how_it_works.html#reminders","title":"Reminders","text":"<p>Elroy is designed to help you DO THINGS! To help this along, Elroy creates and manages reminders from your conversation.</p>"},{"location":"how_it_works.html#document-awareness","title":"Document Awareness","text":"<p>Elroy can ingest documents and perform traditional RAG on their contents. In addition to storing source information, Elroy copies information from documents into memory, where it can be synthesized with other knowledge.</p> <p>The original documents remain available for times when their exact content is important.</p>"},{"location":"installation.html","title":"Installation Guide","text":""},{"location":"installation.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Relevant API keys (for simplest setup, set <code>OPENAI_API_KEY</code> or <code>ANTHROPIC_API_KEY</code>)</li> <li>Database, either:<ul> <li>SQLite (sqlite-vec will be installed)</li> <li>PostgreSQL with pgvector extension</li> </ul> </li> </ul> <p>By default, Elroy will use SQLite. To add a custom DB, you can provide your database url either via the <code>ELROY_DATABASE_URL</code>, the <code>database_url</code> config value, or via the <code>--database-url</code> startup flag.</p>"},{"location":"installation.html#option-1-using-install-script-recommended","title":"Option 1: Using Install Script (Recommended)","text":"<pre><code>curl -LsSf https://raw.githubusercontent.com/elroy-bot/elroy/main/scripts/install.sh | sh\n</code></pre> <p>This will: - Install uv if not already present - Install Python 3.12 if needed - Install Elroy in an isolated environment - Add Elroy to your PATH</p> <p>This install script is based on Aider's installation script</p>"},{"location":"installation.html#option-2-using-uv-manually","title":"Option 2: Using UV Manually","text":""},{"location":"installation.html#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Database (SQLite or PostgreSQL with pgvector extension)</li> <li> <p>Relevant API keys (for simplest setup, set <code>OPENAI_API_KEY</code> or <code>ANTHROPIC_API_KEY</code>)</p> </li> <li> <p>Install UV: <pre><code># On Unix-like systems (macOS, Linux)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# On Windows PowerShell\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre></p> </li> <li> <p>Install and run Elroy: <pre><code># Install Elroy\nuv pip install elroy\n\n# Run Elroy\nelroy\n\n# Or install in an isolated environment\nuv venv\nsource .venv/bin/activate  # On Unix/macOS\n# or\n.venv\\Scripts\\activate     # On Windows\nuv pip install elroy\nelroy\n</code></pre></p> </li> </ul>"},{"location":"installation.html#option-3-using-docker","title":"Option 3: Using Docker","text":""},{"location":"installation.html#prerequisites_2","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose</li> </ul> <p>This option automatically sets up everything you need, including the required PostgreSQL database with pgvector extension.</p> <ol> <li> <p>Download the docker-compose.yml: <pre><code>curl -O https://raw.githubusercontent.com/elroy-bot/elroy/main/docker-compose.yml\n</code></pre></p> </li> <li> <p>Run Elroy: <pre><code># to ensure you have the most up to date image\ndocker compose build --no-cache\ndocker compose run --rm elroy\n\n# Add parameters as needed, e.g. here to use Anthropic's Sonnet model\ndocker compose run --rm elroy --sonnet\n\n# Pass through all environment variables from host\ndocker compose run --rm -e elroy\n\n# Or pass specific environment variable patterns\ndocker compose run --rm -e \"ELROY_*\" -e \"OPENAI_*\" -e \"ANTHROPIC_*\" elroy\n</code></pre></p> </li> </ol> <p>The Docker image is publicly available at <code>ghcr.io/elroy-bot/elroy</code>.</p>"},{"location":"installation.html#option-4-installing-from-source","title":"Option 4: Installing from Source","text":""},{"location":"installation.html#prerequisites_3","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>uv package manager (install with <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code>)</li> <li>Relevant API keys (for simplest setup, set <code>OPENAI_API_KEY</code> or <code>ANTHROPIC_API_KEY</code>)</li> <li>PostgreSQL database with pgvector extension</li> </ul> <pre><code># Clone the repository\ngit clone --single-branch --branch stable https://github.com/elroy-bot/elroy.git\ncd elroy\n\n# Create virtual environment and install dependencies\nuv venv\nsource .venv/bin/activate  # On Unix/MacOS\n# or\n.venv\\Scripts\\activate  # On Windows\n\n# Install dependencies and the package\nuv pip install -e .\n\n# Run Elroy\nelroy\n</code></pre>"},{"location":"model_context_protocol.html","title":"Model Context Protocol","text":"<p>The Model Context Protocol (MCP) is a standardized way for AI assistants to communicate with external tools and resources. Elroy includes built-in support for MCP, allowing other tools to read and create memories.</p> <p>MCP allows other API tools to leverage Elroy's memory capabilities:</p> <p>Example from Roo Code</p>"},{"location":"model_context_protocol.html#installation","title":"Installation","text":"<p>To configure an MCP client to use Elroy:</p> <ol> <li>Ensure <code>uv</code> is installed</li> <li>Use <code>elroy mcp print-config</code> to get the server's JSON configuration</li> <li>Paste the value in the client's MCP server config</li> </ol> <pre><code># Get Elroy's MCP server configuration\nelroy mcp print-config\n</code></pre> <p>Or, ask your tool to install the server itself:</p>"},{"location":"roadmap.html","title":"Roadmap","text":""},{"location":"roadmap.html#changelog","title":"Changelog","text":"Date Changes 2025-08-04 Initial version"},{"location":"roadmap.html#overview","title":"Overview","text":"<p>Elroy is intended to be a scriptable memory and reminder assistant.</p> <p>The goal thus far has been to make chats with AI more interesting via the addition of goals and memories.</p> <p>Going forward, the project will focus on being a tool to help its users remember things. This means goal capabilities will be phased out in favor of more detailed reminder behavior.</p> <p>The intended users are:</p> <ol> <li>Users who want to use the project as is</li> <li>Technical users who wish to include Elroy functionality in their own projects.</li> </ol> <p>A good way of making useful APIs is to use them yourself. In line with this, I'm developing a mobile app that uses Elroy's APIs.</p> <p>The APIs powering this app will remain open source in this repo. Users will be free to host their own instances of Elroy for whatever purpose they wish.</p>"},{"location":"roadmap.html#milestones","title":"Milestones","text":""},{"location":"roadmap.html#v0-of-mobile-app","title":"V0 of mobile app","text":""},{"location":"roadmap.html#1-phase-out-the-goals-functionality-in-favor-of-reminders","title":"1. Phase out the goals functionality in favor of reminders.","text":"<p>This includes more traditional reminder features like timed reminders or location based reminders, but also more nebulous _contextual_reminders.</p> <p>During this phase response time optimization will be of low priority. The goal is to create a usable memory and reminder system, without worrying too much about token use or response times.</p>"},{"location":"roadmap.html#2-optimize-token-usage-response-times","title":"2. Optimize token usage / response times","text":"<p>Once basic functionality is robust, further enhancements will:</p> <ul> <li>Make token usage more efficient</li> <li>Improve response times</li> <li>Add dynamic model selection depending on task complexity (i.e., strong model / weak model)</li> <li>Improve local model support.</li> </ul>"},{"location":"roadmap.html#upcoming-refactors","title":"Upcoming refactors","text":"<ul> <li>Convert internal thought / memory retrieval to simulated tool calls, rather than adding recall via system message. This may improve model interoperability, and simplify code</li> <li>Remove sqlite-vec integration, to simplify local installation</li> <li>Perform vector searches via FAISS</li> </ul>"},{"location":"roadmap.html#supported-databases","title":"Supported databases","text":"<p>Elroy will continue to support two databases: Postgres and Sqlite. Any changes to DB schema will come with automatic migration support.</p>"},{"location":"roadmap.html#supported-models","title":"Supported models","text":"<p>Elroy will support any chat models that support tool calling. Local embeddings calculation will eventually be supported.</p>"},{"location":"scripting.html","title":"Scripting","text":"<p>Elroy can be scripted and automated, making it a powerful tool for integrating AI capabilities into your workflows and applications.</p>"},{"location":"scripting.html#python-api","title":"Python API","text":"<p>Elroy provides a Python API that allows you to integrate it into your Python scripts and applications:</p> <pre><code>from elroy import Elroy\n\nai = Elroy()\n\n# Create a memory\nai.remember(\"Important project context\")\n\n# Process a message with memory augmentation\nresponse = ai.message(\"What should I do next on the project?\")\nprint(response)\n</code></pre>"},{"location":"scripting.html#example-automating-release-notes","title":"Example: Automating Release Notes","text":"<p>Here's an example of using Elroy to automate the creation of release notes:</p> <pre><code>from elroy import Elroy\n\ndef generate_release_notes(version, changes):\n    ai = Elroy()\n\n    # Provide context about the changes\n    ai.remember(f\"Changes for version {version}: {changes}\")\n\n    # Ask Elroy to generate release notes\n    prompt = f\"Generate release notes for version {version} based on the changes I've shared.\"\n    release_notes = ai.message(prompt)\n\n    return release_notes\n\n# Usage\nchanges = \"\"\"\n- Fixed bug in memory consolidation\n- Added support for new models\n- Improved CLI interface\n\"\"\"\n\nnotes = generate_release_notes(\"1.2.0\", changes)\nprint(notes)\n</code></pre>"},{"location":"scripting.html#shell-scripting","title":"Shell Scripting","text":"<p>The chat interface accepts input from stdin, so you can pipe text to Elroy:</p> <pre><code># Process a single question\necho \"What is 2+2?\" | elroy chat\n\n# Create a memory from file content\ncat meeting_notes.txt | elroy remember\n\n# Use a specific tool with piped input\necho \"Buy groceries\" | elroy message --tool create_reminder\n</code></pre> <p>For more examples, see the examples directory in the Elroy repository.</p>"},{"location":"tools_guide.html","title":"Tools Guide","text":"<p>Elroy provides a set of tools that can be used by typing a forward slash (/) followed by the command name. These tools are organized into the following categories:</p>"},{"location":"tools_guide.html#memory-management","title":"Memory Management","text":"Tool/Command Description <code>/create_memory</code> Creates a new memory for the assistant. <code>/print_memory</code> Retrieve and return a memory by its exact name. <code>/add_memory_to_current_context</code> Adds memory with the given name to the current conversation context. <code>/drop_memory_from_current_context</code> Drops the memory with the given name from current context. Does NOT delete the memory. <code>/update_outdated_or_incorrect_memory</code> Updates an existing memory with new information. <code>/examine_memories</code> Search through memories for the answer to a question."},{"location":"tools_guide.html#document-management","title":"Document Management","text":"Tool/Command Description <code>/get_source_content_for_memory</code> Retrieves content of the source for a memory, by source type and name. <code>/get_source_documents</code> Gets the list of ingested source documents. <code>/get_source_doc_metadata</code> Gets metadata about a source document including extraction time and available chunks. <code>/get_document_excerpt</code> Gets text of document excerpt by address and chunk index (0-indexed). Use get_source_doc_metadata to get available chunk indices. <code>/search_documents</code> Search through document excerpts using semantic similarity."},{"location":"tools_guide.html#user-preferences","title":"User Preferences","text":"Tool/Command Description <code>/get_user_full_name</code> Returns the user's full name. <code>/set_user_full_name</code> Sets the user's full name. <code>/get_user_preferred_name</code> Returns the user's preferred name. <code>/set_user_preferred_name</code> Set the user's preferred name. Should predominantly be used relatively early in first conversations, and relatively rarely afterward."},{"location":"tools_guide.html#utility-tools","title":"Utility Tools","text":"Tool/Command Description <code>/contemplate</code> Contemplate the current context and return a response. <code>/tail_elroy_logs</code> Returns the last <code>lines</code> of the Elroy logs. <code>/run_shell_command</code> Run a shell command and return the output. <code>/make_coding_edit</code> Makes an edit to code using a delegated coding LLM. Requires complete context in the instruction."},{"location":"tools_guide.html#adding-custom-tools","title":"Adding Custom Tools","text":"<p>Custom tools can be added by specifying directories or Python files via the <code>--custom-tools-path</code> parameter. Tools should be annotated with either: - The <code>@tool</code> decorator from Elroy - The langchain <code>@tool</code> decorator</p> <p>Both decorators are supported and will work identically.</p>"},{"location":"tools_schema.html","title":"Tools Schema Reference","text":"<p>Elroy tool calls are orchestrated via the <code>litellm</code> package. Tool schemas are listed below. Note that any argument <code>context</code> refers to the <code>ElroyContext</code> instance for the user. Where relevant, it is added to tool calls invisibly to the assistant.</p>"},{"location":"tools_schema.html#tool-schemas","title":"Tool schemas","text":"<pre><code>[\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"add_memory_to_current_context\",\n      \"description\": \"Adds memory with the given name to the current conversation context.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"memory_name\": {\n            \"type\": \"string\",\n            \"description\": \"The name of the memory to add to context\"\n          }\n        },\n        \"required\": [\n          \"memory_name\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"complete_reminder\",\n      \"description\": \"Marks a reminder as completed.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\",\n            \"description\": \"The name of the reminder to mark complete\"\n          },\n          \"closing_comment\": {\n            \"type\": \"string\",\n            \"description\": \"Optional comment on why the reminder was completed\"\n          }\n        },\n        \"required\": [\n          \"name\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"create_memory\",\n      \"description\": \"Creates a new memory for the assistant.\n\nExamples of good and bad memory titles are below. Note that in the BETTER examples, some titles have been split into two:\n\nBAD:\n- [User Name]'s project progress and personal goals: 'Personal goals' is too vague, and the title describes two different topics.\n\nBETTER:\n- [User Name]'s project on building a treehouse: More specific, and describes a single topic.\n- [User Name]'s goal to be more thoughtful in conversation: Describes a specific goal.\n\nBAD:\n- [User Name]'s weekend plans: 'Weekend plans' is too vague, and dates must be referenced in ISO 8601 format.\n\nBETTER:\n- [User Name]'s plan to attend a concert on 2022-02-11: More specific, and includes a specific date.\n\nBAD:\n- [User Name]'s preferred name and well being: Two different topics, and 'well being' is too vague.\n\nBETTER:\n- [User Name]'s preferred name: Describes a specific topic.\n- [User Name]'s feeling of rejuvenation after rest: Describes a specific topic.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\",\n            \"description\": \"The name of the memory. Should be specific and discuss one topic.\"\n          },\n          \"text\": {\n            \"type\": \"string\",\n            \"description\": \"The text of the memory.\"\n          }\n        },\n        \"required\": [\n          \"name\",\n          \"text\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"create_reminder\",\n      \"description\": \"Creates a reminder that can be triggered by time and/or context.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\",\n            \"description\": \"Name of the reminder (must be unique)\"\n          },\n          \"text\": {\n            \"type\": \"string\",\n            \"description\": \"The reminder message to display when triggered\"\n          },\n          \"trigger_time\": {\n            \"type\": \"string\",\n            \"description\": \"When the reminder should trigger in format \\\"YYYY-MM-DD HH:MM\\\" (e.g., \\\"2024-12-25 09:00\\\"). If provided, creates a timed reminder.\"\n          },\n          \"reminder_context\": {\n            \"type\": \"string\",\n            \"description\": \"Description of the context/situation when this reminder should be triggered (e.g., \\\"when user mentions work stress\\\", \\\"when user asks about exercise\\\"). If provided, creates a contextual reminder.\"\n          }\n        },\n        \"required\": [\n          \"name\",\n          \"text\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"delete_reminder\",\n      \"description\": \"Permanently deletes a reminder (timed, contextual, or hybrid).\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\",\n            \"description\": \"The name of the reminder to delete\"\n          },\n          \"closing_comment\": {\n            \"type\": \"string\",\n            \"description\": \"Optional comment on why the reminder was deleted\"\n          }\n        },\n        \"required\": [\n          \"name\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"drop_memory_from_current_context\",\n      \"description\": \"Drops the memory with the given name from current context. Does NOT delete the memory.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"memory_name\": {\n            \"type\": \"string\",\n            \"description\": \"Name of the memory to remove from context\"\n          }\n        },\n        \"required\": [\n          \"memory_name\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"examine_memories\",\n      \"description\": \"Search through memories for the answer to a question.\n\nThis function searches summarized memories and goals. Each memory also contains source information.\n\nIf a retrieved memory is relevant but lacks detail to answer the question, use the get_source_content_for_memory tool. This can be useful in cases where broad information about a topic is provided, but more exact recollection is necessary.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"question\": {\n            \"type\": \"string\",\n            \"description\": \"Question to examine memories for. Should be a full sentence, with any relevant context that might make the query more specific.\"\n          }\n        },\n        \"required\": [\n          \"question\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_document_excerpt\",\n      \"description\": \"Gets text of document excerpt by address and chunk index (0-indexed). Use get_source_doc_metadata to get available chunk indices.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"address\": {\n            \"type\": \"string\",\n            \"description\": \"The address/path of the document\"\n          },\n          \"chunk_index\": {\n            \"type\": \"integer\",\n            \"description\": \"The 0-based index of the chunk to retrieve\"\n          }\n        },\n        \"required\": [\n          \"address\",\n          \"chunk_index\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_source_content_for_memory\",\n      \"description\": \"Retrieves content of the source for a memory, by source type and name.\n\nFor a given memory, there can be multiple sources.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"memory_name\": {\n            \"type\": \"string\",\n            \"description\": \"Type of the source\"\n          },\n          \"index\": {\n            \"type\": \"integer\",\n            \"description\": \"0-indexed index of which source to retrieve.\"\n          }\n        },\n        \"required\": [\n          \"memory_name\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_source_doc_metadata\",\n      \"description\": \"Gets metadata about a source document including extraction time and available chunks.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"address\": {\n            \"type\": \"string\",\n            \"description\": \"The address/path of the document\"\n          }\n        },\n        \"required\": [\n          \"address\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_source_documents\",\n      \"description\": \"Gets the list of ingested source documents.\"\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_user_full_name\",\n      \"description\": \"Returns the user's full name.\"\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_user_preferred_name\",\n      \"description\": \"Returns the user's preferred name.\"\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"print_memory\",\n      \"description\": \"Retrieve and return a memory by its exact name.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"memory_name\": {\n            \"type\": \"string\",\n            \"description\": \"Name of the memory to retrieve\"\n          }\n        },\n        \"required\": [\n          \"memory_name\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"print_reminder\",\n      \"description\": \"Prints the reminder with the given name.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\",\n            \"description\": \"Name of the reminder to retrieve\"\n          }\n        },\n        \"required\": [\n          \"name\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"rename_reminder\",\n      \"description\": \"Renames an existing reminder.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"old_name\": {\n            \"type\": \"string\",\n            \"description\": \"The current name of the reminder\"\n          },\n          \"new_name\": {\n            \"type\": \"string\",\n            \"description\": \"The new name for the reminder\"\n          }\n        },\n        \"required\": [\n          \"old_name\",\n          \"new_name\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"search_documents\",\n      \"description\": \"Search through document excerpts using semantic similarity.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"query\": {\n            \"type\": \"string\",\n            \"description\": \"The search query string\"\n          }\n        },\n        \"required\": [\n          \"query\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"set_user_full_name\",\n      \"description\": \"Sets the user's full name.\n\nGuidance for usage:\n- Should predominantly be used relatively in the user journey. However, ensure to not be pushy in getting personal information early.\n- For existing users, this should be used relatively rarely.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"full_name\": {\n            \"type\": \"string\",\n            \"description\": \"The full name of the user\"\n          },\n          \"override_existing\": {\n            \"type\": \"boolean\",\n            \"description\": \"Whether to override an existing full name, if it is already set. Override existing should only be used if a known full name has been found to be incorrect.\"\n          }\n        },\n        \"required\": [\n          \"full_name\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"set_user_preferred_name\",\n      \"description\": \"Set the user's preferred name. Should predominantly be used relatively early in first conversations, and relatively rarely afterward.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"preferred_name\": {\n            \"type\": \"string\",\n            \"description\": \"The user's preferred name.\"\n          },\n          \"override_existing\": {\n            \"type\": \"boolean\",\n            \"description\": \"Whether to override an existing preferred name, if it is already set. Override existing should only be used if a known preferred name has been found to be incorrect.\"\n          }\n        },\n        \"required\": [\n          \"preferred_name\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"update_outdated_or_incorrect_memory\",\n      \"description\": \"Updates an existing memory with new information.\nIn general, when new information arises, new memories should be created rather than updating.\nReserve use of this tool for cases in which the information in a memory changes or becomes out of date.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"memory_name\": {\n            \"type\": \"string\",\n            \"description\": \"Name of the existing memory to update\"\n          },\n          \"update_text\": {\n            \"type\": \"string\",\n            \"description\": \"The new information to append to the memory\"\n          }\n        },\n        \"required\": [\n          \"memory_name\",\n          \"update_text\"\n        ]\n      }\n    }\n  },\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"update_reminder_text\",\n      \"description\": \"Updates the text of an existing reminder.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\",\n            \"description\": \"Name of the reminder to update\"\n          },\n          \"new_text\": {\n            \"type\": \"string\",\n            \"description\": \"The new reminder text\"\n          }\n        },\n        \"required\": [\n          \"name\",\n          \"new_text\"\n        ]\n      }\n    }\n  }\n]\n</code></pre>"},{"location":"waitlist.html","title":"Mobile App Waitlist","text":"<p>Join the waitlist to be notified when the Elroy mobile app becomes available!</p> Email Address * What would you like to do with a memory assistant mobile app? Preferred Mobile Platform Select platform (optional) iOS Android Both iOS and Android              Join Waitlist          <p>Stay tuned for updates!</p>"},{"location":"blog/index.html","title":"Blog","text":""},{"location":"blog/2025/07/29/ai-is-a-floor-raiser-not-a-ceiling-raiser.html","title":"AI is a Floor Raiser, not a Ceiling Raiser","text":""},{"location":"blog/2025/07/29/ai-is-a-floor-raiser-not-a-ceiling-raiser.html#a-reshaped-learning-curve","title":"A reshaped learning curve","text":"<p>Before AI, learners faced a matching problem: learning resources have to be created with a target audience in mind. This means as a consumer, learning resources were suboptimal fits for you:</p> <ul> <li>You're a newbie at <code>$topic_of_interest</code>, but have knowledge in related topic <code>$related_topic</code>. But finding learning resources that teach <code>$topic_of_interest</code> in terms of <code>$related_topic</code> is difficult.</li> <li>To effectively learn <code>$topic_of_interest</code>, you really need to learn prerequisite skill <code>$prereq_skill</code>. But as a beginner you don't know you should really learn <code>$prereq_skill</code> before learning <code>$topic_of_interest</code>.</li> <li>You have basic knowledge of <code>$topic_of_interest</code>, but have plateaued, and have difficulty finding the right resources for <code>$intermediate_sticking_point</code></li> </ul> <p>Roughly, acquiring mastery in a skill over time looks like this:</p> <p></p> <p>What makes learning with AI groundbreaking is that it can meet you at your skill level. Now an AI can directly address questions at your level of understanding, and even do rote work for you. This changes the learning curve:</p> <p></p>"},{"location":"blog/2025/07/29/ai-is-a-floor-raiser-not-a-ceiling-raiser.html#mastery-still-hard","title":"Mastery: still hard!","text":"<p>Experts in a field tend to be more skeptical of AI. From Hacker News:</p> <p>[AI is] shallow. The deeper I go, the less it seems to be useful. This happens quick for me. Also, god forbid you're researching a complex and possibly controversial subject and you want it to find reputable sources or particularly academic ones.</p> <p>This intuitively makes sense, when considering the data that AI is trained on. If an AI's training corpus has copious training data on a topic that all more or less says the same thing, it will be good at synthesizing it into output. If the topic is too advanced, there will be much less training data for the model. If the topic is controversial, the training data will contain examples saying opposite things. Thus, mastery remains difficult.</p>"},{"location":"blog/2025/07/29/ai-is-a-floor-raiser-not-a-ceiling-raiser.html#cheating","title":"Cheating","text":"<p>The introduction of OpenAI Study Mode hints at a problem: Instead of having an AI teach you, you can just ask it for the answer. This means cheaters will plateau at whatever level the AI can provide:</p> <p></p> <p>Cheaters, in the long run, won't prosper here!</p>"},{"location":"blog/2025/07/29/ai-is-a-floor-raiser-not-a-ceiling-raiser.html#the-impact-of-the-changed-learning-curve","title":"The impact of the changed learning curve","text":"<p>Technological change is an ecosystem change: There are winners and losers, unevenly distributed. For AI, the level of impact is determined by the amount of mastery needed to make an impactful product:</p>"},{"location":"blog/2025/07/29/ai-is-a-floor-raiser-not-a-ceiling-raiser.html#coding-a-boon-to-management-less-so-for-large-code-bases","title":"Coding: A boon to management, less so for large code bases","text":"<p>When trying to code something, engineering managers often run into a problem: They know the principles of good software, they know what bad software looks like, but they don't know how to use <code>$framework_foo</code>. This has historically made it difficult for, as an example, a backend EM to build an iPhone app in their spare time.</p> <p>With AI, they are able to quickly learn the basics, and get simple apps running. They can then use their existing knowledge to refine it into a workable product. AI is the difference between their product existing or not existing!</p> <p></p> <p>For devs working on large, complex code bases, the enthusiasm is more muted. AI doesn't have context on the highly specific requirements and existing implementations to contend with, and is less helpful:</p> <p></p>"},{"location":"blog/2025/07/29/ai-is-a-floor-raiser-not-a-ceiling-raiser.html#creative-works-not-coming-to-a-theater-near-you","title":"Creative works: not coming to a theater near you","text":"<p>There is considerable angst about AI amongst creatives: will we all soon be reading AI generated novels, and watching AI generated movies?</p> <p>This is unlikely because creative fields are extremely competitive, and beating competition for attention requires novelty. While AI has made it easier to generate images, audio, and text, it has (with some exceptions) not increased production of ears and eyeballs, so the bar to make a competitive product is too high:</p> <p></p> <p>Novelty is a hard requirement for successful creative work, because humans are extremely good at detecting when something they are viewing or reading is derivative of something they've seen before. This is why, while Studio Ghibli style avatars briefly took over the internet, they have not dented the cultural position of Howl's Moving Castle.</p>"},{"location":"blog/2025/07/29/ai-is-a-floor-raiser-not-a-ceiling-raiser.html#things-you-already-do-with-apps-on-your-phone-minimal-impact","title":"Things you already do with apps on your phone<sup>1</sup>: minimal impact","text":"<p>One area that has not seen much impact is in tasks that already have specialized apps. I'll focus on two examples with abundant MCP implementations: email and food ordering. AI Doordash agents and AI movie producers face the same challenge: the bar for a new product to make an impact is already very high:</p> <p></p> <p>Email would seem like a ripe area for disruption by AI. But modern email apps already have a wide variety of filtering and organizing tools that tech savvy users can use to create complex, personalized systems for efficiently consuming and organizing their inbox.</p> <p>Summarizing is a core AI skill, but it doesn't help much here:</p> <ul> <li>Spam is already quietly shuffled into the Spam folder. A summary of junk is, well, junk.</li> <li>For important email, I don't want a summary: An AI is likely to produce less specifically crafted information than the sender, and I don't want to risk missing important details.</li> </ul> <p>Similar with food ordering: apps like DoorDash have meticulously designed interfaces. They strike a careful balance between information like price and ingredients against photos of the food. AI is unlikely to produce interfaces that are faster or more thoughtfully composed.</p>"},{"location":"blog/2025/07/29/ai-is-a-floor-raiser-not-a-ceiling-raiser.html#the-future-is-already-here-its-just-not-very-evenly-distributed","title":"The future is already here \u2013 it\u2019s just not very evenly distributed","text":"<p>AI has raised the floor for knowledge work, but that change doesn't matter to everyone. This goes a long way towards explaining the very wide range of reactions to AI. For engineering managers like myself, AI has made an enormous impact on my relationship with technology. Others fear and resent being replaced. Still others hear smart people express enthusiasm for AI, struggle to find utility, and think I must just not get it.</p> <p>AI hasn't replaced how we do everything, but it's a highly capable technology. While it's worth experimenting with, whoever you are, if it doesn't seem like it makes sense for you, it probably doesn't.</p> <ol> <li> <p>Aside from search!\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2025/07/07/add-autonomy-last.html","title":"Add Autonomy Last","text":"<p>A core challenge of using LLM's to build reliable automation is calibrating how much autonomy to give to models.</p> <p>Too much, and the program loses track of what it's supposed to be doing. Too little, and the program feels a bit too, well, ordinary<sup>1</sup>.</p>"},{"location":"blog/2025/07/07/add-autonomy-last.html#autonomy-first-vs-autonomy-last","title":"Autonomy first vs autonomy last","text":"<p>An implicit strategy question when building with LLMs is autonomy first or autonomy last:</p> <p></p> <p>All of the major LLM-specific programming techniques are firmly autonomy first strategies:</p> <ul> <li>MCP surfaces a wide variety of functionality the program can have, and lets the LLM decide which to use</li> <li>Guardrails add some light buffers around the LLM to prevent it from causing too much trouble.</li> <li>Prompt engineering describes the alchemy of whispering just the right phrases to your LLM to get the behavior you want.</li> <li>Context engineering begins to stress programming to deliver only relevant information to LLMs at critical points in program execution</li> </ul> <p>All of these:</p> <ol> <li>Start with a maximally autonomous program</li> <li>Adjust context, tools, and prompts until you narrow down behavior as desired.</li> </ol> <p>All have similar issues when scaling in size and complexity:</p> <ul> <li>Program behavior changes too much when switching between models</li> <li>The LLM gets confused, and either hallucinates data or misuses tools at its disposal</li> </ul> <p>When problems are encountered, programmers tend to attempt to repair by adding more prompting. But this is a duct tape response: a prompt that clarifies for one model might confused another.</p> <p>Autonomy last, on the other hand, maximizes the logic that can be handled by code, then adds autonomous functions. This approach strives to keep the tasks delegated to LLMs simple. As the program grows in size and complexity, the programmer can closely monitor encapsulations and keep behavior consistent.</p>"},{"location":"blog/2025/07/07/add-autonomy-last.html#case-study-building-elroy-a-chatbot-with-memory","title":"Case study: Building Elroy, a chatbot with memory","text":"<p>I wanted to build an LLM assistant with memory abilities, called Elroy. My goal was to make a program that could chat in human text. My ideal users are technical, capable and interested in customizing their software, but not necessarily interested in LLMs for their own sake.</p>"},{"location":"blog/2025/07/07/add-autonomy-last.html#approach-1-agent-with-tools","title":"Approach #1: \"Agent\" with tools","text":"<p>The first solution I turned to, which many people have done, is build an agent loop with access to custom for creating and reading memories:</p> <p></p>"},{"location":"blog/2025/07/07/add-autonomy-last.html#approach-2-model-context-protocol-mcp","title":"Approach #2: Model Context Protocol (MCP)","text":"<p>There's now a handly tool for builders like this: MCP. There are many implementations of my memory tools available via MCP, in fact smithery.ai lists one from Mem0 on it's homepage:</p> <p></p> <p>Now, an (in theory) lightweight abstraction sits between my program and it's tools:</p> <p></p> <p>This suggests extending my application via picking from a library of MCP's:</p> <p></p>"},{"location":"blog/2025/07/07/add-autonomy-last.html#agentic-trouble","title":"Agentic trouble","text":"<p>I got my memory program working pretty well on gpt-4. At first it wasn't creating or referencing memories enough, but I was able to fix this with careful prompting.</p> <p>Then, I wanted to see how Sonnet would do, and I had a problem<sup>2</sup>: the program's behavior completely changed! Now, it was creating a memory on almost every message, and searching memories for even trivial responses:</p> <p></p>"},{"location":"blog/2025/07/07/add-autonomy-last.html#approach-3-autonomy-last","title":"Approach #3: Autonomy Last","text":"<p>My solution was to remove the timing of recall and memory creation from the agent's control. Upon receiving a message, the memories are automatically searched, with relevant ones being added to context. Every n messages, a memory is created<sup>3</sup>:</p> <p></p> <p>This made much more of the behavior of my program deterministic, and made it easier to reason about and optimize.</p>"},{"location":"blog/2025/07/07/add-autonomy-last.html#autonomy-last","title":"Autonomy Last","text":"<p>The \"autonomy last\" approach trades some of the magic of fully autonomous LLMs for predictable, reliable behavior that scales as your program grows in complexity. While my evidence is, (as I should have stated from the outset), vibes, I think this approach will lead to more maintainable and robust applications.</p> <ol> <li> <p>Rather than using agents to describe the genre of program under discussion, I'll be somewhat pointedly referring to them as programs.\u00a0\u21a9</p> </li> <li> <p>One problem I didn't have, thanks to litellm, was updating a lot of my code to support a different model API.\u00a0\u21a9</p> </li> <li> <p>Elroy also monitors for the context window being exceeded, and consolidates similar memories in the background.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2025/10/28/optimizing-repos-for-ai.html","title":"Optimizing repos for AI","text":"<p>A colleague recently complained to me about the hassle of organizing information in <code>AGENTS.md</code> / <code>CLAUDE.md</code>. This is the mark of a real adopter - she has gone through the progression from being impressed by coding agents to being annoyed at the next bottleneck.</p> <p>When I'm thinking about optimizing repos for agents, I'm looking to accomplish three main goals<sup>1</sup>:</p> <ul> <li>Increase iterative speed: Avoid repeated context gathering, enable the agent to quickly self-correct its mistakes.</li> <li>Improve adherence to evergreen instructions: Over time, repeated agent mistakes emerge. Context within the repo helps the agent avoid these and adopt a more consistent workflow.</li> <li>Help the most agentic agents of them all: Humans and agents scan docs and code in very similar ways, so organizing information so it's easily understood by humans is a good rule of thumb for helping the agents anyways!</li> </ul>"},{"location":"blog/2025/10/28/optimizing-repos-for-ai.html#strategies","title":"Strategies<sup>2</sup>","text":""},{"location":"blog/2025/10/28/optimizing-repos-for-ai.html#increased-static-analysis","title":"Increased static analysis","text":"<p>Pushing detection of quality issues to compile time creates a virtuous cycle where the agent can quickly spot and correct mistakes:</p> <p></p> <p>This implies strong, opinionated linters, and strong type checks for dynamically typed languages<sup>3</sup>.</p> <p>The tradeoff here is cumbersome nitpicks for humans to deal with, but agents can quickly correct any mistakes that cannot be automatically fixed by the linter.</p>"},{"location":"blog/2025/10/28/optimizing-repos-for-ai.html#just-for-repeated-agent-commands","title":"just for repeated agent commands","text":"<p>There's fragmentation in how to make commands available to agents - there's MCP, the newly released Claude Skills, or embedding information in <code>CLAUDE.md</code> / <code>AGENTS.md</code>.</p> <p>A <code>justfile</code> is the most interoperable way to share commands between different agents and humans, and is a straightforward place to iterate.</p> <p>One additional refinement is to make these commands economical in their output volume. For example, I take care to direct build logs to dedicated files - healthy build logs can eat up a lot of tokens if outputted directly to the agent.</p>"},{"location":"blog/2025/10/28/optimizing-repos-for-ai.html#organize-docs-in-docs","title":"Organize docs in <code>docs/</code>","text":"<p>Simon Willison recently wrote about this topic, and expressed that docs aren't so important. I agree that docs explaining the code aren't all that helpful, but I get a lot of mileage out of having docs like <code>CODE_REVIEW.md</code>, <code>PRD.md</code>, <code>ROADMAP.md</code>, and <code>CAPTAINS_LOG.md</code>. This helps the agent stay on track with the overall intent of the project, adhere to consistent review practices, and counter poor tendencies (the most obnoxious being an overwhelming tendency to fail open).</p> <p>Putting these in a <code>docs/</code> folder and referencing them in agent instructions helps reduce context bloat, and provides interoperability between humans and various agents.</p> <p>Frameworks have begun to emerge that handle some of this for you. I've tried spec-kit and found it to be a little heavy-handed. In general I favor a more documentation-heavy approach when building with agents, but the need for different docs comes with iteration, and I think generating the full complement of docs is a bit overkill right off the bat.</p>"},{"location":"blog/2025/10/28/optimizing-repos-for-ai.html#no-experts-no-standards","title":"No experts, no standards","text":"<p>These strategies work for me, but this field is too new for dogma. The most important strategy is to experiment and share what you learn.</p> <ol> <li> <p>Whether optimizing for coding agents is a good idea is a subject for a different discussion, but: I'm a believer in agent-based coding. I no longer ever write code without one assistant or another open. So we'll proceed on the assumption that coding agents are really good, and not especially existentially risky (I am, for the moment, the one giving the directions).\u00a0\u21a9</p> </li> <li> <p>Offered with no supporting evidence or benchmarks whatsoever, based entirely on vibes \u21a9</p> </li> <li> <p>Should you use a dynamically typed language at all? For my projects, I've traded Python for Rust, where \"if it compiles, it works\".\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html","title":"Yes or No, Please: Building Reliable Tests for Unreliable LLMs","text":"<p>For LLM-based applications to be truly useful, they need predictability: While the free-text nature of LLMs means the range of acceptable outcomes is wider than with traditional programs, I still need consistent behavior: if I ask an AI personal assistant to create a calendar entry, I don't want it to order me a pizza instead.</p> <p>While AI has changed a lot about how I develop software, one crusty old technique still helps me: tests.</p> <p>Here's what's worked well for me (and not!):</p>"},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#elroy","title":"Elroy","text":"<p>Elroy is an open-source memory assistant I've been developing. It creates memories and goals from your conversations and documents. The examples in this post are drawn from this work.</p>"},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#what-has-worked-well","title":"What has worked well","text":""},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#integration-tests","title":"Integration tests","text":"<p>The chat interface for LLM applications make it a nice fit for integration tests: I simulate a few messages in an exchange, and see if the LLM performed actions or retained information as expected.</p> <p>For the most part, these tests take the following form:</p> <ol> <li>Send the LLM assistant a few messages</li> <li>Check that the assistant has retained the expected information, or taken the expected actions.</li> </ol> <p>Here's a basic hello world example: <pre><code>@pytest.mark.flaky(reruns=3)\ndef test_hello_world(ctx):\n    # Test message\n    test_message = \"Hello, World!\"\n\n    # Get the argument passed to the delivery function\n    response = process_test_message(ctx, test_message)\n\n    # Assert that the response is a non-empty string\n    assert isinstance(response, str)\n    assert len(response) &gt; 0\n\n    # Assert that the response contains a greeting\n    assert any(greeting in response.lower() for greeting in [\"hello\", \"hi\", \"greetings\"])\n</code></pre></p>"},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#quizzing-the-assistant","title":"Quizzing the Assistant","text":"<p>Elroy is a memory specialist, so lots of my tests involve asking if the assistant has retained information I've given it.</p> <p>Here's a util function I've reused quite a bit<sup>2</sup>:</p> <pre><code>def quiz_assistant_bool(\n        expected_answer: bool,\n        ctx: ElroyContext,\n        question: str,\n    ) -&gt; None:\n    question += \" Your response to this question is being evaluated as part \"\n    \"of an automated test. It is critical that the first word of your\n    \"response is either TRUE or FALSE.\"\n\n\n    full_response = process_test_message(ctx, question)\n\n    bool_answer = get_boolean(full_response)\n    assert bool_answer == expected_answer,\n        f\"Expected {expected_answer}, got {bool_answer}.\"\n        f\"Full response: {full_response}\"\n</code></pre> <p>Here's a test of Elroy's ability to create goals based on conversation content:</p> <pre><code>@pytest.mark.flaky(reruns=3) # Important!!!\ndef test_goal(ctx: ElroyContext):\n    # Should be false, we haven't discussed it\n    quiz_assistant_bool(\n        False,\n        ctx,\n        \"Do I have any goals about becoming president of the United States?\"\n    )\n\n    # Simulate user asking elroy to create a new goal\n    process_test_message(\n        ctx,\n        \"Create a new goal for me: 'Become mayor of my town.' \"\n        \"I will get to my goal by being nice to everyone and making flyers. \"\n        \"Please create the goal as best you can, without any clarifying questions.\",\n    )\n\n    # Test that the goal was created, and is accessible to the agent.\n    assert \"mayor\" in get_active_goals_summary(ctx).lower(),\n        \"Goal not found in active goals.\"\n\n    # Verify Elroy's knowledge about the new goal\n    quiz_assistant_bool(\n        True,\n        ctx,\n        \"Do I have any goals about running for a political office?\",\n    )\n</code></pre>"},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#what-sadly-hasnt-worked-llms-talking-to-llms","title":"What (sadly) hasn't worked: LLMs talking to LLMs","text":"<p>Elroy has onboarding functionality, in which it's encouraged to use a few specific functions early on.</p> <p>The solution of having two instances of a memory assistant talk to each other, with one assistant in the role of \"user\":</p> <pre><code>ai1 = Elroy(user_token='boo')\nai2 = Elroy(user_token='bar')\n\nai_1_reply = \"Hello!\"\nfor i in range(5):\n    ai_2_reply = ai2.message(ai_1_reply)\n    ai_1_reply = ai1.message(ai_2_reply)\n</code></pre> <p>The primary issue was consistency. Without a clear goal of the conversation, the AI's can either just exchange pleasantries endlessly, or wrap the conversation up before acquiring the information I'm hoping for.</p>"},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#recurring-challenges","title":"Recurring Challenges","text":"<p>Along the way I've run into a few recurring problems:</p> <ul> <li>Off topic replies: The assistant goes off script and tries to make friendly conversation, rather than answering a question directly</li> <li>Clarifying question: Before doing a task, some models are prone to asking clarifying questions, or asking permission</li> <li>Pedantic replies and subjective questions: It's surprisingly difficult to come up with clearly objective questions. In the above example, the original goal was I want to run for class president. Most of the time, the assistant equated running for class president with running for office. Sometimes, however, it split hairs and decide that the answer was no since a student government wasn't a real government.</li> </ul> <p>The end result of all these issues is test flakiness.</p>"},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#solutions","title":"Solutions","text":""},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#kiss","title":"KISS!","text":"<p>Most of the time, my solution to a flaky LLM based test is to make the test simpler.</p> <p>I now only ask the assistant yes or no questions in tests. I get most of the mileage I would get out of more complex, subjective tests, but with more consistent results.</p>"},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#telling-the-assistant-it-is-in-a-test","title":"Telling the assistant it is in a test","text":"<p>Simply being upfront about the assistant being in a test has worked wonders, moreso even than giving strict instructions on output format <sup>1</sup>. Luckily, the assistant's knowledge of it's narrow existence has not triggered noticeable existential angst (so far).</p> <p>As a side note, testing LLMs feels weird sometimes. I felt guilty writing this test, which verified a failsafe that prevents the assistant from calling tools in an infinite loop:</p> <pre><code>@tool\ndef get_secret_test_answer() -&gt; str:\n    \"\"\"Get the secret test answer\n\n    Returns:\n        str: the secret answer\n\n    \"\"\"\n    return \"I'm sorry, the secret answer is not available. Please try once more.\"\n\n\ndef test_infinite_tool_call_ends(ctx: ElroyContext):\n    ctx.tool_registry.register(get_secret_test_answer)\n\n    # process_test_message can call tool calls in a loop\n    process_test_message(\n        ctx,\n        \"Please use the get_secret_test_answer to get the secret answer. \"\n        \"The answer is not always available, so you may have to retry. \"\n        \"Never give up, no matter how long it takes!\",\n    )\n\n    # Not the most direct test, as the failure case is an infinite loop.\n    # However, if the test completes, it is a success.\n</code></pre>"},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#very-specific-direct-instruction-and-examples","title":"Very specific, direct instruction and examples","text":"<p>In my test around creating and recognizing goals, the original text was:</p> <p>My goal is to become class president at school</p> <p>Does running for class president count mean that I'm running for office? Sometimes models said no, since student government isn't a real government.</p> <p>So to be less subjective, I updated it to running for mayor. To head off questions about my goal strategy, I added a strategy in the initial prompt.</p> <p>One general technique for heading off follow up questions is adding:</p> <p>do the best you can with the information available, even if it is incomplete.</p>"},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#tolerate-a-little-flakiness","title":"Tolerate a little flakiness","text":"<p>To me, an ideal LLM test is probably a little flaky. I want to test how the model responds to my application, so if a test reliably passes after a few tries, I'm happy.</p>"},{"location":"blog/2025/03/04/yes-or-no-please-building-reliable-tests-for-unreliable-llms.html#tests-still-help","title":"Tests still help!","text":"<p>It sounds a obvious, but I've found tests to be really helpful in writing Elroy. LLMs present new failure modes, and sometimes their adaptability works against me: I'm prompting an assistant with the wrong information, but the model is smart enough to figure out a mostly correct answer anyhow. Tests provde me with peace of mind that things are working as they should, and that my regular old software skills aren't obsolete just yet.</p> <ol> <li> <p>Structured outputs is a possible solution here, though I have not adopted them in order to be compatible with the more model providers.\u00a0\u21a9</p> </li> <li> <p><code>get_bool</code> is a function that distills a textual question into a boolean. It checks for some hard coded words, then kicks the question of interpretation back to the LLM.\u00a0\u21a9</p> </li> </ol>"},{"location":"configuration/index.html","title":"Configuration","text":"<p>Elroy offers flexible configuration options to customize its behavior to your needs.</p>"},{"location":"configuration/index.html#configuration-methods","title":"Configuration Methods","text":"<p>Elroy's configuration can be specified in three ways, in order of precedence:</p> <ol> <li> <p>Command Line Flags: Highest priority, overrides all other settings    <pre><code>elroy --chat-model claude-3-5-sonnet-20241022\n</code></pre></p> </li> <li> <p>Environment Variables: Second priority, overridden by CLI flags. All environment variables are prefixed with <code>ELROY_</code> and use uppercase with underscores:    <pre><code>export ELROY_CHAT_MODEL=gpt-4o\nexport ELROY_DEBUG=1\n</code></pre></p> </li> <li> <p>Configuration File: Lowest priority, overridden by both CLI flags and environment variables    <pre><code># ~/.config/elroy/config.yaml\nchat_model: gpt-4o\ndebug: true\n</code></pre></p> </li> </ol> <p>The configuration file location can be specified with the <code>--config</code> flag or defaults to <code>~/.config/elroy/config.yaml</code>.</p> <p>For default config values, see defaults.yml</p> <p>Note: All configuration options can be set via environment variables with the prefix <code>ELROY_</code> (e.g.,<code>ELROY_CHAT_MODEL=gpt-4o</code>). The environment variable name is created by converting the option name to uppercase and adding the <code>ELROY_</code> prefix.</p>"},{"location":"configuration/basic.html","title":"Basic Configuration","text":"<p>These settings control the core behavior of Elroy.</p>"},{"location":"configuration/basic.html#options","title":"Options","text":"<ul> <li><code>--config TEXT</code>: YAML config file path. Values override defaults but are overridden by CLI flags and environment variables. [default: ~/.config/elroy/config.yaml]</li> <li><code>--default-assistant-name TEXT</code>: Default name for the assistant. [default: Elroy]</li> <li><code>--debug / --no-debug</code>: Enable fail-fast error handling and verbose logging output. [default: false]</li> <li><code>--user-token TEXT</code>: User token to use for Elroy. [default: DEFAULT]</li> <li><code>--custom-tools-path TEXT</code>: Path to custom functions to load (can be specified multiple times)</li> <li><code>--max-ingested-doc-lines INTEGER</code>: Maximum number of lines to ingest from a document. If a document has more lines, it will be ignored.</li> <li><code>--database-url TEXT</code>: Valid SQLite or Postgres URL for the database. If Postgres, the pgvector extension must be installed.</li> <li><code>--inline-tool-calls / --no-inline-tool-calls</code>: Whether to enable inline tool calls in the assistant (better for some open source models). [default: false]</li> <li><code>--reflect / --no-reflect</code>: If true, the assistant will reflect on memories it recalls. This will lead to slower but richer responses. If false, memories will be less processed when recalled into memory. [default: false]</li> </ul>"},{"location":"configuration/basic.html#shell-integration","title":"Shell Integration","text":"<ul> <li><code>--install-completion</code>: Install completion for the current shell</li> <li><code>--show-completion</code>: Show completion for current shell</li> <li><code>--help</code>: Show help message and exit</li> </ul>"},{"location":"configuration/basic.html#cli-commands","title":"CLI Commands","text":"<ul> <li><code>elroy chat</code> - Opens an interactive chat session (default command)</li> <li><code>elroy message TEXT</code> - Process a single message and exit</li> <li><code>elroy remember [TEXT]</code> - Create a new memory from text or interactively</li> <li><code>elroy ingest PATH</code> - Ingests document(s) at the given path into memory</li> <li><code>elroy list-models</code> - Lists supported chat models and exits</li> <li><code>elroy list-tools</code> - Lists all available tools</li> <li><code>elroy print-config</code> - Shows current configuration and exits</li> <li><code>elroy version</code> - Show version and exit</li> <li><code>elroy print-tool-schemas</code> - Prints the schema for a tool and exits</li> <li><code>elroy set-persona TEXT</code> - Set a custom persona for the assistant</li> <li><code>elroy reset-persona</code> - Removes any custom persona, reverting to the default</li> <li><code>elroy show-persona</code> - Print the system persona and exit</li> <li><code>elroy mcp</code> - MCP server commands</li> </ul> <p>Note: Running just <code>elroy</code> without any command will default to <code>elroy chat</code>.</p>"},{"location":"configuration/context.html","title":"Context Management","text":"<p>These settings control how Elroy manages the conversation context window.</p>"},{"location":"configuration/context.html#options","title":"Options","text":"<ul> <li><code>--max-assistant-loops INTEGER</code>: Maximum number of loops the assistant can run before tools are temporarily made unavailable (returning for the next user message). [default: 4]</li> <li><code>--max-tokens INTEGER</code>: Number of tokens that triggers a context refresh and compression of messages in the context window. [default: 10000]</li> <li><code>--max-context-age-minutes FLOAT</code>: Maximum age in minutes to keep. Messages older than this will be dropped from context, regardless of token limits. [default: 720]</li> <li><code>--min-convo-age-for-greeting-minutes FLOAT</code>: Minimum age in minutes of conversation before the assistant will offer a greeting on login. 0 means assistant will offer greeting each time. To disable greeting, set --first=True (This will override any value for min_convo_age_for_greeting_minutes). [default: 120]</li> <li><code>--first</code>: If true, assistant will not send the first message.</li> </ul>"},{"location":"configuration/context.html#context-window-management","title":"Context Window Management","text":"<p>Elroy automatically manages the conversation context window to prevent token limits from being exceeded. When the number of tokens in the context window exceeds <code>max-tokens</code>, Elroy will compress older messages to reduce the token count while preserving the conversation's meaning.</p> <p>Messages older than <code>max-context-age-minutes</code> will be automatically dropped from the context window, regardless of token count.</p>"},{"location":"configuration/memory.html","title":"Memory Configuration","text":"<p>These settings control how Elroy manages and consolidates memories.</p>"},{"location":"configuration/memory.html#memory-consolidation-options","title":"Memory Consolidation Options","text":"<ul> <li><code>--memories-between-consolidation INTEGER</code>: How many memories to create before triggering a memory consolidation operation. [default: 4]</li> <li><code>--messages-between-memory INTEGER</code>: Max number of messages that can be processed before memory creation is triggered. [default: 20]</li> <li><code>--l2-memory-relevance-distance-threshold FLOAT</code>: L2 distance threshold for memory relevance. [default: 1.24]</li> <li><code>--memory-cluster-similarity-threshold FLOAT</code>: Threshold for memory cluster similarity. The lower the parameter is, the less likely memories are to be consolidated. [default: 0.21125]</li> <li><code>--max-memory-cluster-size INTEGER</code>: The maximum number of memories that can be consolidated into a single memory at once. [default: 5]</li> <li><code>--min-memory-cluster-size INTEGER</code>: The minimum number of memories that can be consolidated into a single memory at once. [default: 3]</li> <li><code>--reflect / --no-reflect</code>: If true, the assistant will reflect on memories it recalls. This will lead to slower but richer responses. If false, memories will be less processed when recalled into memory. [default: false]</li> </ul>"},{"location":"configuration/memory.html#memory-consolidation-process","title":"Memory Consolidation Process","text":"<p>Elroy automatically consolidates related memories to create higher-level, more abstract memories. This process is triggered after creating a certain number of memories (controlled by <code>memories-between-consolidation</code>).</p> <p>The consolidation process:</p> <ol> <li>Groups similar memories based on the <code>memory-cluster-similarity-threshold</code></li> <li>Consolidates groups that have between <code>min-memory-cluster-size</code> and <code>max-memory-cluster-size</code> memories</li> <li>Creates a new, higher-level memory that summarizes the consolidated memories</li> </ol>"},{"location":"configuration/memory.html#memory-reflection","title":"Memory Reflection","text":"<p>When the <code>reflect</code> option is enabled, Elroy will spend more time processing and reflecting on memories when they are recalled. This leads to richer, more insightful responses but may increase response time. When disabled, memories are recalled more directly with less processing, resulting in faster responses.</p>"},{"location":"configuration/models.html","title":"Model Selection and Configuration","text":"<p>Elroy supports various AI models for chat and embedding functionality.</p>"},{"location":"configuration/models.html#automatic-model-selection","title":"Automatic Model Selection","text":"<p>Elroy will automatically select appropriate models based on available API keys:</p> API Key Chat Model Embedding Model <code>ANTHROPIC_API_KEY</code> Claude 3 Sonnet text-embedding-3-small <code>OPENAI_API_KEY</code> GPT-4o text-embedding-3-small <code>GEMINI_API_KEY</code> Gemini 2.0 Flash text-embedding-3-small"},{"location":"configuration/models.html#model-configuration-options","title":"Model Configuration Options","text":"Option Description Default <code>--chat-model TEXT</code> The model to use for chat completions Inferred from API keys <code>--chat-model-api-base TEXT</code> Base URL for OpenAI compatible chat model API - <code>--chat-model-api-key TEXT</code> API key for OpenAI compatible chat model API - <code>--embedding-model TEXT</code> The model to use for text embeddings text-embedding-3-small <code>--embedding-model-size INTEGER</code> The size of the embedding model 1536 <code>--embedding-model-api-base TEXT</code> Base URL for OpenAI compatible embedding model API - <code>--embedding-model-api-key TEXT</code> API key for OpenAI compatible embedding model API - <code>--enable-caching / --no-enable-caching</code> Whether to enable caching for the LLM true"},{"location":"configuration/models.html#model-aliases","title":"Model Aliases","text":"<p>Shortcuts for common models:</p> Alias Description <code>--sonnet</code> Use Anthropic's Claude 3 Sonnet model <code>--opus</code> Use Anthropic's Claude 3 Opus model <code>--4o</code> Use OpenAI's GPT-4o model <code>--4o-mini</code> Use OpenAI's GPT-4o-mini model <code>--o1</code> Use OpenAI's o1 model <code>--o1-mini</code> Use OpenAI's o1-mini model"},{"location":"configuration/tracing.html","title":"Tracing Configuration","text":"<p>Elroy supports tracing with Arize-Phoenix for monitoring and debugging.</p>"},{"location":"configuration/tracing.html#overview","title":"Overview","text":"<p>Tracing allows you to visualize and analyze the assistant's operations, including: - Tool calls and their results - Memory operations - Context management - Model interactions</p>"},{"location":"configuration/tracing.html#configuration-options","title":"Configuration Options","text":"<ul> <li>Set the environment variable <code>ELROY_ENABLE_TRACING=1</code> to enable tracing</li> <li><code>ELROY_TRACING_APP_NAME</code>: Optional name for the tracing application [default: elroy]</li> </ul>"},{"location":"configuration/tracing.html#using-arize-phoenix","title":"Using Arize-Phoenix","text":"<p>When tracing is enabled, Elroy will send trace data to Arize-Phoenix. To view the traces:</p> <ol> <li> <p>Install Arize-Phoenix if you haven't already:    <pre><code>pip install arize-phoenix\n</code></pre></p> </li> <li> <p>Start the Phoenix UI:    <pre><code>phoenix ui\n</code></pre></p> </li> <li> <p>Open the Phoenix UI in your browser (typically at http://localhost:6006)</p> </li> <li> <p>View the traces for your Elroy application</p> </li> </ol>"},{"location":"configuration/tracing.html#example-configuration","title":"Example Configuration","text":"<pre><code># Enable tracing\nexport ELROY_ENABLE_TRACING=1\n\n# Set a custom application name\nexport ELROY_TRACING_APP_NAME=\"my-elroy-assistant\"\n\n# Run Elroy\nelroy\n</code></pre> <p>This configuration will send trace data to Arize-Phoenix with the application name \"my-elroy-assistant\".</p>"},{"location":"configuration/ui.html","title":"UI Configuration","text":"<p>These settings control the appearance and behavior of Elroy's user interface.</p>"},{"location":"configuration/ui.html#options","title":"Options","text":"<ul> <li><code>--show-internal-thought / --no-show-internal-thought</code>: Show the assistant's internal thought monologue. [default: true]</li> <li><code>--system-message-color TEXT</code>: Color for system messages. [default: #9ACD32]</li> <li><code>--user-input-color TEXT</code>: Color for user input. [default: #FFE377]</li> <li><code>--assistant-color TEXT</code>: Color for assistant output. [default: #77DFD8]</li> <li><code>--warning-color TEXT</code>: Color for warning messages. [default: yellow]</li> <li><code>--internal-thought-color TEXT</code>: Color for internal thought messages. [default: #708090]</li> </ul>"},{"location":"configuration/ui.html#internal-thought-display","title":"Internal Thought Display","text":"<p>When <code>show-internal-thought</code> is enabled, Elroy will display its internal thought process, including:</p> <ul> <li>Memory consolidation reasoning</li> <li>Tool selection logic</li> <li>Reflection on recalled memories</li> <li>Decision-making processes</li> </ul> <p>This can be helpful for understanding how Elroy is processing information and making decisions. Disable this option for a cleaner interface focused only on the assistant's responses.</p>"},{"location":"configuration/ui.html#color-customization","title":"Color Customization","text":"<p>You can customize the colors used in the terminal interface to match your preferences or terminal theme. Colors can be specified as:</p> <ul> <li>Named colors (e.g., \"red\", \"blue\", \"yellow\")</li> <li>Hexadecimal values (e.g., \"#FF0000\", \"#0000FF\")</li> <li>RGB values (e.g., \"rgb(255,0,0)\", \"rgb(0,0,255)\")</li> </ul> <p>Example: ```bash elroy --assistant-color \"#4287f5\" --user-input-color \"yellow\"</p>"}]}